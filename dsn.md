# Data Science Notes

# Fundamentals

## SQL

The general structure of a query is:

```
with local_var as (â€¦)
select c.CustomerName, count(o.OrderID) as n_orders
from Customers as c
left join Orders as o
on c.CustomerID = o.CustomerID
where o.OrderDate >= "1997-01-01"
group by c.CustomerName
having n_orders > 0;
```

Below are two more complex examples.

You have a table named *users* with two fields:

- id 
- friend_id

The following request gives the number of common friends between each pair of users, in ascending order:


```
with unique_ids as (
        select distinct id
        from users
     ),
     couples as (
        select list1.id as id1, list2.id as id2 
        from unique_ids as list1 
        cross join unique_ids as list2 
        where list1.id < list2.id
     )
select	
    id1, 
    id2, 
    (select count(*) 
     from (select count(t.friend_id) as cc 
           from users t 
           where t.id in (id1, id2) 
           group by t.friend_id 
           having cc > 1)) as number_of_cc
from couples
order by number_of_cc asc; 
```

You have a table named *logs* with two fields:

- id
- time

The following request gives the users who where churned but resuscitated on 2010-06-12:

```
with churned as (
    select id 
    from logs 
    where time <= datetime('2010-06-11') 
    group by id 
    having julianday(max(time)) < julianday('2010-06-11') - 30
    )
select id
from logs 
where time >= datetime('2010-06-12') and id in churned
group by id 
having min(time) = datetime('2010-06-12');
```

## Matrix decomposition and pseudo-inverse

Square matrices may be diagonalizable by EVD (Eigen Value Decomposition):

- $A = P S P^{-1}$
- P is the matrix of eigen vectors in columns
- S is the diagonal matrix of eigen values $\lambda_i$, in descending order by convention
- The decomposition is unique only if eigenvalues are unique

Real symmetric matrices are diagonalizable by EVD:

- $A = Q S Q^T$
- Q is real and orthogonal
- S is real
- The effect of A is scaling space by $\lambda_i$ in direction $q_i$. 

Non square (n x p) real matrices are diagonalizable by SVD (Singular Value Decomposition):

- $A = U S V^T$
- U is the (n x n) orthogonal matrix of left singular vectors
- S is the (n x p) non square diagonal matrix of singular values
- V is the (p x p) orthogonal matrix of right singular vectors
- Can be obtained by diagonalization of $AA^T$ and $A^TA$ 

Non square matrices are pseudo invertible:

- $A^+ = \lim_{\alpha \to 0} (A^TA + \alpha I)^{-1}A^T$ by definition
- $A^+ = V S^+ U^T$ is the practical way of computing it
- U, S, V are the svd of A
- S^+ is the transpose of S with non zero elements being inverted

## Probability and information theory

$X$ a vector of random variables, its covariance matrix is:

$$Var(X) = Cov(X, X) = E[(X-EX)(X-EX)^T] = E[XX^T] - EX(EX)^T$$

$X$ is a data matrix (samples in line), its sample covariance matrix is:

$$\hat{Var}(X) = \frac 1 {n-1} X^T (Id-1/n)^2 X$$

If $X$ is centered then:

$$Var(X) = E[XX^T] \quad \text{and} \quad \hat{Var}(X) = \frac 1 {n-1} X^TX$$


Todo : linear and non linear change of variable.


Characteristic function of a distribution $f$:

$$\phi_f(w) = \int f(x)e^{iwx} dx \quad \text{and} \quad E[X^k] = \frac 1 {i^k} \frac {\partial^k \phi} {\partial w^k}(0)$$


Bayes:

$$P(w|X) = \frac {P(X|w)P(w)} {P(X)} = \frac {P(X|w)P(w)} {\int_w P(X|w)P(w)} $$


Total expectation:

$$E \big[ E[X|Y,Z] \big] = E[X]$$

and

$$E \big[ E[X|Y,Z]|Y \big] = E[X|Y]$$

in particular we can decompose:

$$E[X]= \sum_y p(y) E[X|Y=y] $$


For $X \geq 0$ and $t \geq 0$ we have:

$$P(X \geq t) \leq \frac {E[X]} {t} \quad \text{ (Markov)}$$

For $X \sim \mu, \sigma$ we have:

$$P(|X-\mu| > k \sigma) <= \frac 1 {k^2} \quad \text{ (Chebyshev)}$$

For $X_i \sim \mu, \sigma$ identically distributed and $S_n = \frac 1 n \sum_1^n X_i$ we have:

- if pairwise correlated with correlation $\rho$

$$\frac {Var(S_n)}  {\sigma^2} = \rho + \frac {1-\rho}{n}$$

- if independent

$$S_n \xrightarrow{as} \mu \quad \text{and} \quad \frac {S_n-\mu} {\sigma / \sqrt n} \xrightarrow{d} N(0, 1)$$

- if independent normal with bessel-corrected sample variance $\hat{\sigma}^2$

$$\frac {S_n-\mu} {\hat{\sigma} / \sqrt n} \sim t_{n-1}$$


Entropy:

|Name                       |Formula        |
|------                     |-----          |
|Entropy                    |$H(p) = - E_p\log p$|
|Cross entropy              |$H(p, q) = - E_p \log q$|
|Kullback-Leibler divergence|$KL(p \| q) = -E_p \log q/p = H(p, q) - H(p)$

NB:

$$KL(p \| q) \geq 0, \quad \text{equality if and only if p = q almost everywhere}$$

## Frequentist statistics

Data are considered random iid, generated by a distribution with fixed parameters $w$.

Frequentists focus on the likelihood of the data $L(w|X) = P(X|w)$

Point estimate or statistic : $\hat{w} = g(X)$

MLE estimator: $\hat{w} = argmax_w P(X|w)$


Bias of estimator: $Bias(\hat{w})= E[\hat{w}] - w$

MSE of estimator: $E[(\hat{w} - w)^2] = Bias(\hat{w})^2 + Var(\hat{w})$


Examples:

- Sample mean: $\hat{\mu} = \frac 1 n \sum_1^n x_i$

- Unbiased sample variance: $\hat{\sigma} = \frac 1 {n-1} \sum_1^n (x_i - \hat{\mu})^2$

The estimator distribution is called the sampling distribution.
It can be theoretically deducted from the data distribution or approximated by bootstrap, as its name implies.

Statistics are also used to test the hypotheses made on the data:

1. State the relevant null ($H_0$) and alternative hypothesis ($H_1$).

1. Consider the statistical assumptions being made about the sample in doing the test (independence, form of the distributions...)

1. Decide which test is appropriate and state the relevant test statistic T.

1. Derive, considering the assumptions, the distribution of T under $H_0$. 

1. Select a significance level ($\alpha$), a probability threshold below which the null hypothesis will be rejected.

1. Compute the observed value $t$ of the test statistic T.

1. Calculate the p-value. This is the probability, under the null hypothesis, of sampling a test statistic at least as extreme as that which was observed.

1. Reject $H_0$, in favor of $H_1$, if and only if the p-value is less than the significance level.

Note that the p-values, if $H_0$ is true, are uniformly distributed in [0, 1]. 

Proof: we have $Pval(t) = P(T < t) = F_T(t)$, so under $H_0$, $Pval \sim F_T(T) \sim U(0, 1)$.

Examples:

- one sample z-test (test if population mean = $\mu$ with large population or known variance)
- one sample t-test (test if population mean = $\mu$)
- two sample independent t-test (test if two independent samples have the same mean)
- two sample paired t-test (test if two paired samples have the same mean, similar to one sample t-test)
- anova (test if 3 or more independent samples have the same mean)
- chi squared test for categorical data goodness of fit or independence
- Fisher exact test for 2x2 categorical data

NB : anova is equivalent to linear regression with categorical features.

To evaluate the normality assumption quantile-quantile plots are a good start, completed by statistical tests.

todo:permutation tests

## Bayesian statistics

Bayesian statistics take the view that the data is fixed and the parameters random. Instead of focusing on the likelihood $P(X|w)$ it is focusing on the posterior distribution given by Bayes formula: 

$$P(w|X) = \frac {P(X|w)P(w)} {P(X)} = \frac {P(X|w)P(w)} {\int_w P(X|w)P(w)}$$

The evidence $P(X)$ being independent of $w$, the main theoretical difference being frequentists and bayesians is the consideration of the prior $P(w)$.

Once the posterior distribution is estimated, prediction for new data $X'$ can be made:

$$P(X'|X) = \int P(X'|w) P(w|X) dw$$

In practice this complete computation is hard, and it is common to replace the posterior $P(w|X)$ by a dirac at its maximum $w_{MAP}$, turning the prediction into a standard point estimate prediction $P(X'|w_{MAP})$. In this case bayesian prediction is equivalent to frequentist MLE, with a regularization factor equal to the prior $P(w)$.

## Sampling

Suppose we have the distribution $p(x)$ that we know how to evaluate but not sample from.

In the trivial case when we know the cdf $F$ and the quantile function $F^{-1}$, we can sample from $U \sim U(0, 1)$ and then apply $F^{-1}$ to the sample. This works because if we have the cdf $F$ and the rv $U \sim U(0, 1)$, then the rv $F^{-1}(U) \sim F$.

We can also sample $x$ from a sample-able distribution $q$ and then reject some samples randomly proportionally to the ratio $p(x)/q(x)$, therefore adjusting the density of samples to $p$. This is called rejection sampling, it works better in low dimensions with $q$ close to $p$.

Another possibility is to get $L$ samples from $q$ and then resample from them with a weight $p(x_i)/q(x_i)$, this is sampling importance resampling.

Gibbs sampling is used for multivariate distributions, it works by sampling sequentially each variable at a time, using the 1D probability of this variable conditionned on all others. The gibbs sequence of samples should be subsampled so high correlation is removed. 

Metropolis Hasting is a generalization of Gibbs and rejection sampling, where we generate the sequence from q and then reject probabilistically to adjust on $p$. 


## Optimization

Newton's method for finding the zeros of a function f:

$$w := w - \frac {f(w)}{f'(w)}$$

Newton's method for the optimum of a function f:

$$w := w - \frac {f'(w)}{f''(w)}$$

Newton-Raphson for multidimensional optimum of f:

$$w := w - H_w^{-1} \nabla_w f(w)  $$


## Machine learning generalities

Most machine learning algorithms can be viewed as:

- a model $P(X|w)$ for the data
- a loss function $L(x_i, w)$, to evaluate the fit of the model to individual data samples
- a regularization term $\Omega(w)$ 
- an optimization technique to minimize the cost function
$J(w) = \sum_i L(x_i, w) + \Omega(w)$

The "standard" frequentist loss function is the negative log likelihood:

$$NLL(X, w) = -\log p(X|w) = - \log \prod_i p(x_i|w) = -\sum_i \log p(x_i|w)$$

The NLL is equivalent to the cross entropy between the empirical distribution $p_{emp} = \sum_i \frac 1 n \delta_{x_i}$ and the model distribution $p(x|w)$:

$$\frac 1 n NLL(X, w) = -E_{p_{emp}} \log p = H(p_{emp}, p) = KL(p_{emp}, p) + H(p_{emp})$$

$p_{emp}$ being fixed by the data, NLL is also equivalent to the KL divergence between the empirical and model distribution. 

In supervised cases the standard is the conditional cross entropy loss is $L(x, y, w) = -\log p(y|x, w)$

Example of loss functions for regression:

- L2
- L1

Example of loss functions for classification where $y = +- 1$, $\hat y = sign(z)$:

- $\log(1 + \exp(-yz))$ : logistic or log loss, equivalent to cross entropy in the case of logistic regression
- $[1 - yz]_+$ : Hinge
- $\exp(-yz)$ : Exponential


The generalization performance of the model is evaluated on a test set, not used for optimization (training), the different variants are:

- hold-out cross validation (80% train / 20% test)
- k-fold cross validation (10 times 90% / 10%)
- leave-one-out cross validation (k = n)

When tuning hyperparameters or comparing many models, an additional "dev set" is needed, to avoid overfitting the test set. Make sure dev and test sets come from the same distribution.

If you have a large dataset (1M samples) you may need only 1% for dev and 1% for test.

Simple machine learning recipe (solve first problem first):

|Problem|Solutions|
|---|---|
|High bias (training error >> bayes error)|More parameters / train longer / change model|
|High variance (dev error >>  train error)|More data / regularization / change model|


Feature selection can be done in a generic way:

- forward selection: start with 0 features and add the best feature at each step
- backward selection: start with all features and remove the worst feature at each step
- filter selection: select the k features with the best correlation score with the target, estimated on the training set

Correlation scores can be:

- $MI(x, y) = KL(p(x, y)||p(x)p(y))$, mutual information
- chi2
- cramer V (normalised chi2)


Variable importance can be evaluated in a model agnostic way, by permuting the values of the variable between different samples, and measure the effect on performance.

When you have trained a supervised model $p(z|x, y) = f(x, y)$, you can use partial dependences to plot the effect of x and y separately.

If you have a multivariate function $f(x, y)$, the partial dependence on x tries to extract the x component of the function, by averaging over the value of y. If $f$ is additive or multiplicative it will work perfectly.

$$f_X(x) = E[f(x, Y)] = \int f(x, y)p(y)dy$$

Unless X and Y are independent, it is not to be confused with the conditional expectation given x, which tries to estimate $f(x, y)$ using only x, by averaging over the values of y that are likely given x:

$$g_X(x) = E[f(X, Y)  |X=x] = \int f(x, y)p(y|x)dy$$

In particular if $f(x, y) = E[Z|X=x, Y=y]$, then $E[Z|X=x]$ is not the partial dependance on x, it is the conditional expectation.

Averaged over all possible data-generating distributions, every classification algorithm has the same error rate on new data points. This is called the no free lunch theorem. In the real world some data distributions are more common, and it is possible to design algorithms that perform better on these distributions.

## Representer theorem and kernel models

http://cs229.stanford.edu/notes/cs229-notes-all/representer-function.pdf

## Learning theory

http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes4.pdf

## EM algorithm

The EM is a general algorithm to maximize the likelihood $p(X|w)$ in a model with latent variable $Z$.

It is supposed that, being a latent variable model, direct maximization of $p(X|w)$ is hard and $p(X, Z|w)$ is easy.

To decompose the maximization we introduce the distribution $q(Z)$ and rewrite the log likelihood:

$$\log p(X|w) = E_q \log \frac {p(X,Z|w)}{q(Z)} + KL(q(Z), p(Z|X, w))$$

Proof: $\log P(X|w) = E_q \log P(X|w) = E_q \log \frac {P(X, Z|w) q(Z)}{P(Z|X, w)q(Z)} = E_q \log \frac {P(X, Z|w)}{q(Z)} + E_q \log \frac {q(Z)}{P(Z|X, w)}$

Then we optimize sequentially on $q(Z)$ and $w$:

- E-step:
    - we minimize the second term down to 0 by setting $q(Z) = p(Z|X, w)$
    - the log likelihood does not change (it does not depend on $q(Z)$)
    - the first term is maximized up to the log likelihood
- M-step: 
    - we maximize the first term on $w$
    - the second term increases (it is a KL starting at at 0)
    - the log likelihood increases

The EM can be view as coordinate ascent on the first term, which is a lower bound of our objective.

Further reading: 

- Bishop
- [http://cs229.stanford.edu/notes/cs229-notes8.pdf]

# Supervised models

## Ordinary least squares linear regression (OLS)


$$y_i \sim \mathcal{N}(w^T x_i, \sigma^2), iid$$
$$\mu_i = w^T x_i = E[y_i|x_i]$$
$$P(y_i | x_i, w)= (2\pi\sigma^2)^{-1/2} exp[-(y_i-\mu_i)^2/(2\sigma^2)]$$

The cost function is:

$$NLL(w) = -\sum_i \log P(y_i | x_i, w) \sim \sum_i (y_i-\mu_i)^2 + cte
         \sim RSS(w) + cte$$

$$\hat{w} = \hat{w}_{OLS} =  argmin_w RSS(w)$$

In vector form (vector y and design matrix X with samples stacked in lines):

$$y \sim \mathcal{N}(Xw, \sigma^2 I)$$
$$\mu = Xw$$
$$RSS(w) = (y-\mu)^T(y-\mu)$$
$$RSS'(w) = -2X^T(y-\mu)$$

If the sample covariance is invertible we have the normal equation:

$$\hat{w} = (X^TX)^{-1} X^Ty$$

and therefore

$$\hat{w} \sim \mathcal{N}(w, (X^TX)^{-1} \sigma^2 I)$$

$$\sigma^2 \approx \hat{\sigma}^2 = \frac{RSS(\hat{w})}{N-p-1} $$

This allows for t-test of the $\hat{w}_j$:

$$t_j = \frac{\hat{w}_j}{\hat{\sigma} \sqrt{((X^TX)^{-1})_{jj}}} \sim t_{N-p-1}$$

To test a group of parameters together:

$$F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \sim F stat$$

## Locally weighted regression (LWR)

Instead of fitting $\sum_i (y_i - w^T x_i)^2$ globally, we fit $\sum_i \lambda_i (y_i - w^T x_i)^2$ for each query point $x$, where $\lambda_i$ is the weight for sample i, for example 

$$\lambda_i = \exp - \frac {(x_i-x)^2} {2 \tau^2}$$

so that local samples matter more in the prediction of $x$.

LWR is a non parametric method, as the whole dataset is needed at prediction time.

## Ridge regression

With L2 regularization or gaussian prior on $w$:

$$\hat{w}_{ridge} = (X^TX + \lambda I)^{-1} X^Ty$$

todo: explain link with PCA

## Logistic regression

$$ y_i \sim Ber(\sigma(w^Tx_i)), iid \in {0, 1}$$
$$ \mu_i = \sigma(w^T x_i) = P(y_i = 1|x_i) = E[y_i|x_i]$$
$$ P(y_i | x_i, w) =\mu_i^{y_i} (1-\mu_i)^{1-y_i} $$
$$ NLL(w) = -\sum_i \log P(y_i | x_i, w) = -\sum_i [y_i \log \mu_i + (1-y_i)\log(1-\mu_i)]$$

$$\hat{w} =  argmin_w NLL(w) $$

$$NLL'(w) = -X^T(y-\mu)$$


## Softmax regression

$$y \sim Cat(\mu_k)$$
$$P(y | x, w) = \prod \mu_k^{[y = k]} $$
$$\mu_k = \frac {\exp w_k^T x}{\sum_l \exp w_l^T x}$$


## GLM

Remember the linear and logistic regression models:

- $y|x \sim \mathcal{N} (\mu, \sigma^2)$
- $y|x \sim Ber (\mu)$
 
GLMs are generalizations of these, where the output density is in the exponential family:

$$p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta))$$

- $\eta$ : natural/canonical parameter
- $T(y)$ : sufficient statistic, often the identity
- $a(\eta)$ : log partition constant, serves as normalizer so probability integrates to 1.

Below we show that the bernouilli and gaussian are examples of the exponential family.

The bernouilli distribution can be written:

$$\mu^y (1-\mu)^{1-y} = \exp (y \log \frac \mu {1-\mu} + \log(1-\mu))$$

so:

- $\eta = \log \frac \mu {1-\mu}$ or $\mu = \sigma(\eta)$
- $T(y) = y$
- $a(\eta) = - \log(1-\mu) = \log(1 + e^{\eta})$
- $b(y) = 1$

The gaussian with unit variance can be written:

$$\frac 1 {\sqrt{2\pi}} \exp (- \frac {(y - \mu)^2} 2 ) = \frac 1 {\sqrt{2\pi}} \exp (-y^2 / 2) \exp(\mu y - \mu^2 / 2)$$

so:

- $\eta = \mu$
- $T(y) = y$
- $a(\eta) = \mu^2 / 2 = \eta^2/2$
- $b(y) = \frac 1 {\sqrt{2\pi}} \exp (-y^2 / 2)$

Common examples from the exponential family:

- normal
- bernouilli and multinouilli (categorical data)
- binomial and multinomial with fixed number of trial (count data)
- negative binomial with fixed number of failure
- poisson (count data)
- gamma, exponential (non negative data like intervals)
- beta and dirichlet (data in [0, 1] like probabilies)

Not in the family:

- student
- most mixture distributions
- uniform distributions with variable bounds

A GLM is a model where:

- $y|x;w \sim ExpFamily(\eta)$
- $\hat y = E[y|x;w] = g(\eta)$ (in the frequent case where T = identity)
- $\eta = w^T x$

$g(\eta)$ is called the canonical response function, its inverse is called the canonical link function.

## Gaussian Discriminant Analysis (GDA)

Generative model where the data given the class is gaussian. 

If the covariance matrix is the same for each class the decision boundary is linear -> LDA.

Otherwise it is quadratic -> QDA.

[Generative models, GDA, Naive Bayes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes2.pdf)

## Naive bayes

Generative model for categorical features, with the assumption that they are independent given the class.

Two variants:

- one Bernouilly for each word in the dictionary
- one Multinouilli for each word in the document

A version for continuous features exist : Gaussian NB, which is just LDA with a diagonal covariance matrix.

Note that the term "bayes classifier" refers to the generic classifier who chooses the class with max probability.

[Generative models, GDA, Naive Bayes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes2.pdf)

## Neural Network

Forward propagation:

- $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l-1]}$
- $A^{[l]} = g^{[l]}(Z^{[l]})$

Back propagation:

- $dZ^{[l]} = dA^{[l]} * {g'}^{[l]}(Z^{[l]})$
- $dW^{[l]} = \frac 1 m dZ^{[l]} A^{[l-1]T}$
- $db^{[l]} = \frac 1 m sum(dZ^{[l]}, axis=1)$
- $dA^{[l-1]} = W^{[l]T} dZ^{[l]}$

With:

- $X = A^{[0]}$: design matrix transposed (samples as columns)
- $\hat Y = A^{[L]}$: estimated output
- $W^{[l]} : (n^{[l]}, n^{[l-1]})$
- $b^{[l]} : (n^{[l]}, 1)$
- $Z^{[l]}, A^{[l]}: (n^{[l]}, m)$
- $l$ : layer index
- $m$ : number of samples
- $n^{[l]}$ : dimension of layer l (0 for input layer, L for output layer) 

Inputs should be normalized so the cost function is more "round" and easier to optimize by gradient descent (larger steps). Decorrelation can also help.

Dropout regularization:

- Set activations to 0 with probability R
- Divide the remaining activations by 1-R to compensate the drop in average activation
- R can be higher on larger layers more prone to overfitting
- Only at training time
- Very useful in computer vision
- Makes the cost / step graph harder to analyse

Early stopping:

- Stop when dev error starts to increase
- Acts as regularization (weights stay close to the linear zone)
- Mixes optimization and regularization that are normally independant

Weight initialization:

- We want to avoid vanishing or exploding gradients
- The weight of layer l should be $N(\mu = 0, \sigma^2 = \frac k n)$
- $n$ beeing the size of the [l-1] layer, ie the number of inputs to this layer
- k = 1 for tanh (Xavier initialization), 2 for relu (He initialization).

Gradient checking:

- Reshape all your parameters into one $w$
- Compute the approximate gradient $dw_a$ with centered schema
- Compute $\frac {|dw_a - dw|} {|dw_a| + |dw|}$, it should be around 10E-7
- Disable drop-out while gradient checking
- Can be run after some training, so that weights are far from 0

Minibatch:

- Split the dataset in packet of 512
- Run vectorized gradient descent on each packet
- All the packets = 1 epoch
- Much faster than full batch on large datasets
- Batch size = m -> batch GD, stable but slow
- Batch size = 1 -> SGD, less stable, faster, but not optimally vectorized
- Not necessarily converging, but improve on average
- If m < 2000, use batch size = m else use size 64, 128, 256, 512, should fit in memory

Momentum:

- Exponential averaging of $dW$
- $\bar {dW} = 0$
- $\bar {dW} = \beta \bar {dW} + (1 - \beta) dW$
- Roughly equivalent to averaging the first $\frac 1 {1-\beta}$ terms
- Optional bias correction: $\frac {\bar {dW}} {1 - \beta^t}$
- Typically $\beta = 0.9$

RMSprop:

- Exponential averaging of $dW^2$ with hyper-parameter $\beta_2$
- Optional bias correction
- Divide $dW$ by $\sqrt{\bar {dW^2}} + \epsilon$

Adam:

- RMSprop scaling applied to momentum $\bar{dW}$
- With bias correction
- Typically $\beta = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$
- Works well on many use cases

Learning rate decay:

- $t$ = epoch number 
- $\alpha / \alpha_0 = (1 + rt)^{-1}$ or $r^t$ or $rt^{-1/2}$

Local optima:

- Local optima are not really a problem, as most stationary points are saddle points in high dimension
- Plateau area are more problematic, as they can slow down learning significantly

Inportance of hyper parameters tuning:

1. $\alpha$
1. $\beta$, mini batch size, #hidden units
1. #layers, learning rate decay
1. adam parameters

Use random search, from coarse to fine, with logarithm scale for $\alpha$, $1-\beta$.

If you have a lot of compute power use // training, otherwise use "baby sitting" approach to tuning.

Batch normalization:

- Replace the preactivation $z$ by $\gamma \frac {z - \mu}{\sigma} + \beta$
- $\mu, \sigma$ are computed on the minibatch
- $\gamma$ and $\beta$ are learnables parameters like $b$ (same dimension), for each layer
- $b$ actually becomes redundant with $\beta$ and can be set to zero
- Makes learning in a layer easier by limiting the changes in the preceding layers
- Because the normalization is done on minibatches, it introduces a bit of noise in the activations like dropout, so it has a small regularization effect
- An estimate of $\mu, \sigma$ is needed at test time, it is computed by exponentially averaging the $\mu, \sigma$ of each mini batch.

## Convolutional Neural Network

Convolution layer:

- Compared to dense layers, convolution provides parameter sharing, sparsity of connections and tranlation invariance
- A (f x f) filter is applied simply by elementwise multiplication and sum on the (n x n) image, reducing its size by f-1 in each dimension.
- To avoid image size reduction, a padding of p null pixels can be added around the image increasing each dimension by 2p
- Valid convolution: padding p = 0, output size is n-f+1
- Same convolution: padding p = (f-1)/2, output size is n, f is generally odd
- Stride s is the step for applying the filter, final size is (n+2p-f)/s + 1, rounded down
- It is not mathematically a convolution as the filter should be reversed, it does not matter in the context of CNN
- If the image has nc channels (r g b), the filter should have nc channels as well
- Adding filters will add outputs:  6x6x3 image convoluted with two 3x3x3 filters will generate a 4x4x2 output
- Eventually we had a bias term to each output and apply the activation function. We end up with a standard formulation $z = W * a^{[-1]} + b$ and $a = g(z)$

Notations for layer $l$:

- $f^{[l]}$: filter size
- $p^{[l]}$: padding
- $s^{[l]}$: stride
- $n_c^{[l]}$: number of filters
- Input $a^{[l-1]}$ of size $n^{[l-1]} \times n^{[l-1]} \times n_c^{[l-1]}$
- One filter size $f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$
- Weight size $f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$
- Bias size $n_c^{[l]}$
- Output $a^{[l]}$ of size $n^{[l]} \times n^{[l]} \times n_c^{[l]}$
with $n^{[l]} = \lfloor (n^{[l-1]} + 2p^{[l]} - f^{[l]})/s^{[l]} \rfloor + 1$
- Activations for whole dataset $A^{[l]}$ of size $m \times n^{[l]} \times n^{[l]} \times n_c^{[l]}$

Pooling:

- Take max instead of weighted sum
- No weights to learn 
- Applies to each channel
- Common choice f=s=2, p=0
- Averaging instead of max is possible
- Generally not considered a layer, but part of the preceding conv layer

Classic networks

- LeNet 5
- AlexNet
- VGG 16

Residual Networks

- Shortcut / skip connection are added: $a^{[l]} = g(z + a^{[l-2]})$
- Less vanishing or exploding gradient problems
- Can learn identity easily
- Allows to train deeper networks

Network in network or 1x1 convolution

- A 1x1 convolution with multiple filters, applied to a multichannel input acts as fully connected layer
- A way to reduce the number of channels without changing the image size

Inception networks

- Use multiple filters or max pooling that output the same image size, and stack them as channels
- Botteleneck layers are used to reduce the size and computational cost

Advice for implementation:

- If you want to reproduce an architecture, lookup the open source implementation on github

Transfer learning

- Get a trained network that works well on a task similar to yours
- Freeze all or parts of the weights
- Change the softmax end layer and train it
- The more data you have the less weights you freeze
- Almost always useful

Data augmentation

- Mirroring
- Random cropping
- Rotation
- Shearing
- Color shifting
- PCA color augmentation
- Perform them on the fly, after data loading and before training

Tips for doing well on benchmarks or competitions

- ensembling of several independently trained NN
- 10-crop : run classifier on 10 crops of the image and average
- start with pre-trained open source implementation

## Image detection

Classification with localization

- output is (object_is_present, bounding box params, proba of categories)
- $y = [p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$ 
- if object not present, the loss ignores the full output

Landmark Detection

- output is (object_is_present, landmark1_x, landmark1_y...)
- position of eyes, chin, shoulder...

Sliding window detection

- Pick a window size
- Slide the window through the image and apply the classifier convnet
- Increase window size and repeat
- Expensive computationally
- Can be computed faster in one convnet by replacing FC layers by 1x1 conv layers, so the output is a grid of results for each sliding layer
- Not extremelly accurate

YOLO algo to improve accuracy

- Split the image in 19x19 subimages and define a 8 dimension classification with localization output for each subimage, the bounding box being first the position of the center, and then the height and width relative to the subimage dimensions 
- Build a convnet to predict the 19x19x8 outputs
- For each subimage you extract the bounding box if present
- Hard paper to read !

Evaluation of localization

- IoU, intersection over union of the boxes > 0.5 is considered good

Avoid duplicate detections

- Suppose output is (pc, bx, by, bh, bw) for each 19x19 subimage
- Discard all boxes with pc < 0.6
- Repeat until no more boxes:
    - Pick the box B with max pc
    - Discard boxes overlaping with B (IoU > 0.5)  

Overlapping objects

- We can detect 2 objects of different shape in the same cell by defining 2 anchor boxes, one vertical for pedestrian, one horizontal for cars
- Each object in training is assigned to a cell (cell contains midpoint) AND an anchor box (highest IoU with bounding box)
- Anchor boxes may be chosen automatically by clustering the set of boxes

Face detection

- Siamese network $f(Image)$ gives encoding
- Anchor image, Positive image, Negative image: $A, P, N$
- Triplet loss $L(A, P, N) = \max(d(A, P) - d(A, N) + \alpha , 0)$
- Training set: 10k pictures of 1k people
- Choose A, P, N so that $d(A, P) \approx d(A, N)$, so the learning is more effective
- See DeepFace and FaceNet 
- Instead of the triplet loss, a binary classifier can be learned to compare the encodings from the siamese networks: $\hat y = \sigma(\sum_k w_k |f(x)_k - f(x')_k| + b)$

## Sequence models

Take the problem of NER in a phrase. It does not fit very well into a standard DNN because:

- inputs / outputs can be of different length $T_x <> T_y$ 
- features can be learned accross different positions of text
- the number of parameters would be huge

RNN have the following structure:

```
        | a0 = 0
        v
x1 -> |ooo| -> y1
        | a1
        v
x2 -> |ooo| -> y2
        | a2
        v
x3 -> |ooo| -> y3
        | a3
        v
x4 -> |ooo| -> y4
        .
        .
        .
```

Bidirectional RNN can use information before and after a particular step.

The forward propagation is formally:

$$a^{<1>} = g(W_a [a^{<0>}, x^{<1>}] + b_a)$$

$$\hat y^{<1>} = g(W_y a^{<1>} + b_y)$$


- The parameters W are shared accross all time steps
- The brackets indicate vertical stacking of vectors
- g is generally tanh, can be relu

Loss is $\sum_t L(\hat y^{<t>}, y^{<t>})$ where L is the logistic loss $- \sum_i y_i \log \hat y_i$

Different RNN architectures:

- Tagging, $T_x = T_y$: many-to-many
- Machine translation, $T_x \neq T_y$: many-to-many
- Sentiment classification, only the last y is outputed: many-to-one
- Music generation, only the first x is inputed: one-to-many
- Standard DNN: one-to-one

Language modelling with RNN:

- Tokenize: map words to a dictionary, including UNK and EOS
- $x^{<t>} = y^{<t-1>}$ is the preceding word at t
- $y^{<t>}$ is the actual word at t
- $\hat y^{<t>} = p(y^{<t>}|y^{<t-1>}, y^{<t-2>}...)$
- The product of $\hat y^{<t>}$ gives the probability of the sentence
- Works at character level too. No need for a dictionary. Harder to train and to capture long dependencies. 
- Can be used easily for text generation

Vanishing gradient

- Deep networks and RNN are affected by vanishing gradients (counpounding of weights)
- Hard to influence the end of the sentence from the beginning of the sentence, which is a problem for grammar (subject -> verb)
- Exploding gradient is also possible and easier to handle (gradient clipping)

Gated Recurrent Unit (simplified)

- A memory cell c is kept at time t (it replaces the activation a)
- A new proposed memory cell is computed
- A gate controls if the update is used or if we stay with the old value
- Memory can be a vector, in this case the gate is also a vector

$$\Gamma_u = \sigma(W_u [c^{<t-1>}, x^{<t>}] + b_u)$$

$$\tilde{c}^{<t>} = \tanh(W_c [c^{<t-1>}, x^{<t>}] + b_c)$$

$$c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}$$

$$a^{<t>} = c^{<t>}$$

Gated Recurrent Unit

- Update and relevance gate

$$\Gamma_u = \sigma(W_u [c^{<t-1>}, x^{<t>}] + b_u)$$

$$\Gamma_r = \sigma(W_r [c^{<t-1>}, x^{<t>}] + b_r)$$

$$\tilde{c}^{<t>} = \tanh(W_c [\Gamma_r c^{<t-1>}, x^{<t>}] + b_c)$$

$$c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}$$

$$a^{<t>} = c^{<t>}$$

LSTM

- $a \neq c$
- Update, forget and output gates

$$\Gamma_u = \sigma(W_u [a^{<t-1>}, x^{<t>}] + b_u)$$

$$\Gamma_f = \sigma(W_f [a^{<t-1>}, x^{<t>}] + b_f)$$

$$\Gamma_o = \sigma(W_r [a^{<t-1>}, x^{<t>}] + b_r)$$

$$\tilde{c}^{<t>} = \tanh(W_c [a^{<t-1>}, x^{<t>}] + b_c)$$

$$c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + \Gamma_f * c^{<t-1>}$$

$$a^{<t>} = \Gamma_o * \tanh c^{<t>}$$


GRU are more recent but actually a simplification of LSTM. They are a bit more scalable because more simple.

BRNN

- Each step gets a forward and a backward activation
- Prediction gets the concatenated forward and backward activations as input
- Cannot make a prediction before the the end of the sequence is read (not great for speech recognition)

Deep RNN

- Instead of having just one activation between x and y, several activation layers can be stacked in between
- Only a few layers are used in practice


Word embeddings

- Embeddings carry more information than one-hot
- Can be visualised by t-SNE
- They allow you to do transfer learning (use embeddings trained on huge datasets for your small dataset task)
- Embeddings arithmetic works: man - woman is similar to king - queen
- Cosine similarity: scalar product divided by norms

Embedding matrix

- E is (300 x 10000), containing the embeddings of each word in columns
- $E o_j = e_j$, $o_j$ being the one-hot encoding of word j (but using 
multiplication is slow in practice)

Learning embeddings

- The embedding matrix is included and learned in a language model 
(predicting the next word) 
- Embeddings can be more efficiently learned within a dedicated task, rather 
than an accurate language model
- Skip gram : predict a target word with only one nearby word as input

Word2vec

- Simple model to predict the target: embedding matrix -> softmax
- Softmax is slow because 10,000 vocabulary terms in the denominator
- Hierarchical softmax (tree of binary classifiers) scales in log|V|
- Negative sampling
- Mikolov

Negative sampling

- Instead of predicting the target word, we predict if a given 
(context c, target t) is coherent (the target is in the context)
- The positive samples are taken from the training sentences
- For each positive sample, k negative samples are generated by picking 
random words as targets

GloVe word vectors

- Simply optimize for the embeddings $\theta$ and $e$ so that scalar product is close to 
log of frequency of co-appearence
- Then take the average of $\theta$ and $e$, as they play a symmetric role

Sentiment classification

- Average the embeddings of the words in the sentence and then softmax
- RNN many-to-one

Debiasing word embeddings

1. Identify bias direction by averaging he - she, male - female etc...
1. Project every non definitional word to get rid of bias
1. Equalize grandmother - grandfather = girl - boy = ...
 
 
Attention model...

## Deep Learning Project Management

Orthogonalization

- Think of analog TV tuning, we want one effect only per control

Chain of ML assumptions and possible tuning:

1. fit training set well (bigger network, adam...)
1. fit dev set (regularization, bigger training set)
1. fit test set (bigger dev set)
1. does well in real world (change dev set or cost function)

Early stopping affects step 1 and 2, so not so easy to tune.

Metrics:

- Choose a single number evaluation metric, and possibly a satisficing metric (eg running time <= 100ms)
- N metrics : 1 optimizing, N-1 satisficing
- Example : optimize accuracy st # false positive <= 1 per day

Train/dev/test

- dev and test set should come from the *same* distribution
- should reflect data you expect to get in the future
- dev and test set size = min(m/5, 10000)
- test set may be 0, if you dont care and can test in real

When to change dev/test set or metrics

- Model A is better according to metric but not according to humans ( maybe a class is more important -> weight the accuracy )
- Model A does better on dev/test but not on real data 

Human level performance

- improvment tends to slow down above human level
    - because of bayes optimal perf
    - because you can no longer ask humans for help!
- human level is a good proxy for bayes error
- aim for best human level error on your training set (remove avoidable bias), then work on dev set (remove variance)
- hard to beat on perception tasks

Summary

```
Human-level
^
|  Avoidable bias <- bigger model/train longer/optimizer/archi
v
Training error
^
|  Variance <- more data/data augmentation/regularization
v
Dev error
^
|  Dev set overfit <- more data in the dev set
v
Test error
```

Error analysis is key

- look at 100 mislabeled examples
- if 5 of mislabeled exampled are type A, working on type A is not necessarily the best use of your time
- count the type of problem for each 100 mislabeled example 
- problem can come from the dataset (wrong label)
- work on the most common problems

Cleaning up wrong labels in data set

- if the errors are reasonably random, it may be ok to leave them
- if the error is systematic (eg all white dogs are labelled cats) it is more important to fix it
- compute the contribution of wrong labels in the total error, to see if you should work on fixing labels
- make sure the dev and test sets remain equally distributed
- train set may remain uncorrected, algos are robust to that
- consider examining the correctly identified examples as well 

Build the first system quickly, then iterate

- unless you have a lot of previous experience or litterature on the pb

Training and testing on different distributions

- You may have less examples from the distribution you really care about (the one closest to real data)
- So distributing them proportionally (say 90/5/5) in train/dev/test would under-represent them in the dev/test
- Better to over-represent them in the dev/test sets (say 50/25/25), even if that breaks the training / dev similarity
- In this case you may want to add a training-dev set, extracted from the training set, so you have a dev set with the same distribution as training, and you can distinguish variance error from change of distribution error (data mismatch)

Data mismatch

- try manual error analysis to understand the difference between training and dev/test sets
- for example, your dev/test sets may contain "in car background noise", so you can try simulating this kind of background noise in your training set
- when synthetizing data, be careful not to make the training data too specialized (eg by using only one hour of car noise to add to 10,000 hours of speech, or using image synthesis software), thus impoverishing the training data and you will probably overfit


```
Human-level
^
|  Avoidable bias <- bigger model/train longer/optimizer/archi
v
Training error
^
|  Variance <- more data/data augmentation/regularization
v
Train-Dev error
^
|  Data mismatch <- more data/data synthesis in train
v
Dev error
^
|  Dev set overfit <- more data in the dev set
v
Test error
```

Transfer learning

- change the last layer of your network
- retrain only the last layer
- if you have enough data you may retrain more layers
- if you have enough data to retrain the whole network from scratch, you don't need transfer learning

Multi-task learning

- learning several tasks (multi label) at the same time may work better than multiple single task learning
- task may share low level features
- works better is amount of data is similar in each task
- you need enough data to train one big NN
- not used very often, mostly on computer vision 

End to end deep learning

- direct mapping from x to y in constrast to traditional pipelined approach
- no need for hand designed components and kwnoledge
- can work better if you have enough data:
    - speech recognition: sound -> transcript
    - translation: english -> french
- not always better
    - identification = face detection + face identification
    - self driving = object detection + motion planning + control


## Support Vector Machines

Here we present the binary SVM classifier.

SVM does not naturally fit in a probabilistic context, the classification is directly given by:

$$\hat y = sign(w^T x + b)$$

Instead of minimizing a loss, SVM is expressed as maximizing the margin between the separating hyperplane and the data points, which can be rewritten as:

$$\min_{w, b} \frac 1 2 \|w\|^2 + C \sum_i \epsilon_i$$

$$\text{s.t. } y_i(w^T x_i + b) \geq 1-\epsilon_i, \epsilon_i \geq 0$$

Introducing the Lagrangian:

$$\mathcal L(w, b, \epsilon, \alpha, r) = \frac 1 2 w^T w + C \sum_i \epsilon_i - \sum_i \alpha_i [y_i(x_i^T w + b) -1 + \epsilon_i] - \sum_i r_i \epsilon_i$$

and deriving w.r.t. $w$ we find that:

$$w = \sum_i \alpha_i y_i x_i$$

and the dual form is:

$$\max_{\alpha} \sum \alpha_i - \frac 1 2 \sum_{i,j} y_i y_j \alpha_i \alpha_j (x_i^T x_j)$$

$$\text{s.t. } 0 \leq \alpha_i \leq C, \sum_i \alpha_i y_i=0$$

so we can express the optimization in terms of cross products, as well as the prediction:

$$\hat y = sign (\sum_i \alpha_i y_i (x_i^T x) + b)$$

allowing us to use the "kernel trick".

According to the KKT dual complementary condition, only a few $\alpha_i$ will be non zero, at the support vectors, allowing for a fast evaluation of the sums.

Finally, it can be shown that SVM can be reframed as:

$$\min \sum_i L(y_i, w^T x_i + b) + \lambda \|w\|^2$$

with L as the hinge loss:

$$L(y, z) = \max(0, 1 - yz)$$

which makes it similar to logistic regression that uses the log loss:

$$L(y, z) = \log(1 + \exp(- yz))$$



Further reading:
- [http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes3.pdf]

## Decision Tree

The DT model is:

$$T(x, w) = \sum_m c_m 1[x \in R_m]$$

where regions $R_m$ are constructed by recursively adding separating hyperplanes of the form $x_j = s$ into each region.

The loss can be decomposed by regions:

$$J(w) = \sum_i L(x_i, y_i, w) = \sum_m \sum_{x_i \in R_m} L(y_i, c_m)$$

Given the regions, the $c_m$ can be trivially computed by $c_m = arg \min_c \sum_{x_i \in R_m} L(y_i, c)$ (average for MSE loss, median for MAE loss, majority vote for classification loss...), but finding the optimal regions is not computationally feasible, so it is done by greedily choosing the best split at each step. 

In practice the $c_m$ are implicited and the objective used to drive the splits is:

$$\tilde J(w) = \sum_m N_m H(\{y_i | x_i \in R_m\})$$

where H is a measure of impurity or randomness like gini or entropy (we want to minimize impurity), computed on the class frequencies $p_k$ (Gini is $\sum_k p_k(1-p_k)$, similar to entropy). 
Misclassification rate is generally not used because it is not differentiable and is indifferent to error repartition (gini and entropy will favor pure nodes, for convexity reasons). 

When a region R is splitted in R+ and R-, the objective decrease proportionally to the "information gain", so this is the criterion that is maximized for choosing the split:

$$IG = H(R) - \frac{N^+}N H(R^+) - \frac{N^-}N H(R^-)$$

Regularization can take many forms (total number of nodes, minimum of samples per node, depth, threshold on decrease...) and can be a constraint or integrated in the objective.

Regularization can happen during tree construction (stop one branch from growing further) or after (pruning from the leaves). Stopping early can prevent beneficial splits to be made later down the branch.

Trees are high variance low bias.

Further reading:

- ESL
- [misclassification error vs entropy](https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html)


## Random Forest

RF is bagging (bootstrap aggregating) applied to trees:

---
- for b = 1 to B:
    - draw a bootstrap sample Z from X, of the same size
    - create root tree $T_b$
    - for each leaf in $T_b$
        - select m < p variables at random
        - select the best split variable and split point for Z
        - split the leaf into two new leaves
        - stop if size($T_b$) >= min_size
    - store $T_b$
- $f(x) = \frac 1 B \sum_b T_b(x)$
---

For classification the class probability (proportion of each class in the selected leaf) are averaged.

Bagging works well here because trees are low bias and high variance. Bagging reduces the variance without compromising the bias. If the trees are identically distributed with correlation $\rho$:

$$ Var(f) / Var(T) = \rho  + \frac {1-\rho}B$$

A nice aspect of RF is that it provides "out of bag" error estimate, which can serve as cross validation error estimate. This error can be computed on the fly if tree are built sequentially.

Trees can be built in //, so training can be fast.

For a given variable, its importance is the sum of all the information gain generated by the choice of this variable in a tree, average on all trees. It may be scaled so the sum of importances over variables sum to 1. 

RF is relatively simple and its performance is very robust.

RAFR:

- ESL

## Gradient Boosted Trees

Gradient boosting can be seen as an algorithm for stage-wise additive modeling with arbitrary loss functions.

The stage-wise additive modeling algorithm is:

---
- Initialize $f_0(x)=0$
- For m = 1 to M:
    
$$ (\alpha_m, \beta_m) = arg \min_{\alpha, \beta} \sum_i L(y_i, f_{m-1}(x_i) + \alpha B(x_i, \beta)) $$

$$f_m(x) = f_{m-1}(x) + \alpha_m B(x_i, \beta_m)) $$
---

Depending on the loss function and the base learner B, the minimization above can be hard or not implemented. 

Gradient boosting replaces this potentially hard minimization by a simple L2 regression on the direction of the steepest descent of the objective:

   $$(\alpha_m, \beta_m) = argmin_{\alpha, \beta} \sum_i \Big[ \alpha B(x_i, \beta) - \frac{\partial L(y_i, c)}{\partial c} (f_{m-1}(x_i)) \Big]^2$$ (1)

Now that $\alpha_m B(X, \beta_m)$ points in the right direction, we can easily adjust $\alpha_m$ by line search on the real objective:

$$ \alpha_m = argmin_{\alpha} \sum_i L(y_i, f_{m-1}(x_i) + \alpha B(x_i, \beta_m)) $$

Note that the standard technique of fitting the new learner on the residuals $y_i - f_{m-1}(x_i)$ corresponds to the gradient of the L2 loss, so using it instead of the real gradient would mainly ignore the real loss in the base learner construction, except for the final scaling.

Gradient boosted trees apply this to trees:

---
- Initialize $f_0(x) = argmin_c \sum_i L(y_i, c)$
- For m = 1 to M:
    - For each data point i, compute 
    
    $$r_{im} = - \frac{\partial L(y_i, c)}{\partial c} (f_{m-1}(x_i))$$

    - Fit a new L2 regression tree $T_m$ of size J on $\{x_i, r_{im}\}$
    - For each leaf j of $T_m$, adjust the predicted value to optimize the real loss
    
    $$c_{jm} = argmin_c \sum_{x_i \in R_{jm}} L(y_i, f_{m-1}(x_i) + c)$$

    - Update $f_m(x) = f_{m-1}(x_i) + \nu T_m$
---

The hyper-parameters are:

- $J$: the size of the base tree
- $\nu$: the shrinkage / learning rate  for regularization in the update step
- $M$: the number of trees, the optimal value can be estimated by monitoring the test error during training

Obviously $\nu$ and $M$ interact strongly.

Stochastic gradient boosting borrows from random forest: it is the same as GBT but each new tree is learned on a subsample (without replacement).

Further reading:

- ESL
- https://statweb.stanford.edu/~jhf/ftp/trebst.pdf
- https://xgboost.readthedocs.io/en/latest/tutorials/model.html


## AdaBoost

# todo


# Unsupervised models

## Kernel density estimation

We have data $X \sim p$ and we want to build an estimator $\hat p$ from it.

Kernel estimation is a generalization of the histogram:

$$\hat p(x) = \frac 1 {nh} \sum_i K(\frac {x_i -x}h)$$

The smoothing parameter h may be chosen by minimizing the mean integrated squared error (MISE) of the estimator:

$$E_p \int(\hat p - p)^2(x) dx = E_p\int\hat p^2 - 2E_p\int\hat p p + E_p\int p^2$$
$$= E_p\int\hat p^2 - 2E_p[\hat p] + cte$$

which can be estimated without bias by the cross validation estimator:

$$J(h) = \int\hat p^2 - \frac 2 n \sum_i \hat p_{-i}(x_i) + cte$$

where $\hat p_{-i}$ is the estimator build on all points minus the point i.

## Principal Component Analysis

We want to project the data on a smaller dimension space while keeping the maximum variance, or equivalently, linearly encode and decode the dataset $X$ into a smaller dimensional dataset $Z$ with minimal L2 reconstruction error.

The data should be normalized first to avoid overweighting the large variables. Note that normalization may affect the sparcity of matrices.

Supposing the decoder is $R = ZW^T$, W columns being orthonormal, we can prove that the optimal encoder is $Z = XW$, so the optimal reconstruction is:

$$R = XWW^T$$

Then we can prove that the optimal matrix $W_t$ is  the truncated matrix of eigen vectors of $X^TX$, keeping only the vectors with the largest eigen values from:

$$X^T X = W S W^T$$

Note that $Z=XW_t$ so 
$\hat{Var}(Z) = \frac 1 {n-1} Z^T Z = \frac 1 {n-1} W_t^T X^T X W_t = \frac 1 {n-1} W_t^T W S W^T W_t = \frac 1 {n-1} S_t$. 
So the components of z are uncorrelated.

The same result can be obtained by SVD of X:

$$X = U \Sigma W^T \rightarrow X^TX = W \Sigma^2 W^T$$

$$Z = X W_t = U \Sigma_t$$

EVD on $X^T X$ is different numerically from SVD on $X$. The first method may be less stable (square operation) but faster (matrix is reduced to p x p).

In scikit-learn, truncated SVD is recommended for sparse one-hot encoded matrices, as the PCA implementation automatically normalize the data and would make it dense.

## Fisher LDA

Fisher LDA is actually a supervised technique, generalized as LDA / QDA but it can be used as a projection similar to PCA. It is a projection that maximizes the class separation in the projected space:

$$w \sim (\Sigma_0 + \Sigma_1)^{-1} (\mu_1 - \mu_0)$$

## Factor Analysis

See [http://cs229.stanford.edu/notes/cs229-notes9.pdf]

## Independent Component Analysis

See [http://cs229.stanford.edu/notes/cs229-notes11.pdf]

## K Means clustering

1. Initialize cluster centroids
1. Repeat until convergence:
    - assign each point to its closest centroid
    - move each centroid to the mean of its newly assigned points

This procedure is equivalent to coordinate descent on:

$$J(c, \mu) = \sum_i \| x_i - \mu_{ci}\|$$

See [http://cs229.stanford.edu/notes/cs229-notes7a.pdf]


## Gaussian Mixture Model

E-step:

$$w_j^{(i)} := p(z^{(i)} = j | x^{(i)}; \phi, \mu, \sigma) \sim p(x^{(i)} | z^{(i)} = j) p(z^{(i)} = j)$$

M-step:

$$\phi_j := \frac 1 m \sum_i w_j^{(i)}$$

$$\mu_j := \frac {\sum_i w_j^{(i)} x^{(i)}} {\sum_i w_j^{(i)}}$$

$$\sigma_j := \frac {\sum_i w_j^{(i)} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T} {\sum_i w_j^{(i)}}$$

See [http://cs229.stanford.edu/notes/cs229-notes8.pdf]


## T-SNE

todo

## Hierarchical clustering

Agglomerative (bottom-up) or divisive (top-down).

Agglomerative starts with each point as its own cluster and agglomerates the most similar clusters.

Similarity is defined by a metric and a linkage criteria:

- min distance
- average distance
- increase in variance when cluster are merged
- ...

[https://en.wikipedia.org/wiki/Hierarchical_clustering]

## DBSCAN clustering

Scan the points and make 3 categories:

- core points : they have at least M points in their epsilon-neighborhood
- edge points : they have one core point in their epsilon-neighborhood
- noise points : the rest

Clusters are made of connected core points and their edge points

Complexity $n \log n, n$

[https://en.wikipedia.org/wiki/DBSCAN]


## Latent Dirichlet Allocation

# Time Series Analysis

Stochastic process:

- $X_t \sim P_t(\mu_t, \sigma^2_t)$

Stationarity:

- no systematic change in mean and variations
- no periodicity

Autocovariance:

- $\gamma(s, t) = Cov(X_s, X_t)$
- $\gamma(t, t) = \sigma^2_t$

Stationarity implies:

- $\gamma(t, t+k) = \gamma_k \approx c_k = \frac 1 N \sum_t (x_t - \bar x)(x_{t+k} - \bar x)$

Autocovariance:

- $\gamma_k$
- $c_k$ (sample)

Autocorrelation:

- $\rho_k = \gamma_k / \gamma_0$
- $r_k = c_k / c_0$ (sample)

Partial correlation:

- Correlation of X and Y partialling out Z (controling for Z)
- Correlation of the residual after regressing on Z

Partial autocorrelation:

- Autocorrelation of $X_t$ and $X_{t+k}$ partialling out the variables in between

ACF and PACF:

- autocorrelation function and partial acf

AR(p=2):

- $X_t = Z_t + \phi_{t-1} X_{t-1} + \phi_{t-2} X_{t-2}$
- Not necessarily stationary
- ACF will tail off
- PACF will cut after lag 2
- Equivalent as infinite order MA (use backshift operator and expand 1/(1-x))

MA(q=2):

- $X_t = Z_t + \theta_1 Z_{t-1} + \theta_2 Z_{t-2}$
- ACF will cut after lag 2
- PACF will tail off
- Equivalent to infinite order AR

ARMA(p, q):

- ACF and PACF will tail off 
- Try several p and q
- Compare with AIC

ARIMA(p, d, q):

- $(1-B)^d X_t \sim ARMA(p, q)$
- differentiation can remove the trend and make the serie stationary (cf random walk)

SARIMA $(p, d, q, P, Q, D)_s$:

- dependency on t-12 for example in monthly data (s=12)

Modeling:

- Plot
- Trend suggests differencing
- Variation in variance suggest transformation (eg log)
- ACF suggests order of MA(q) and SMA(Q)
- PACF suggests order of AR(p) and SAR(P)
- AIC, MSE to compare models with parsimony (sum of 6 hyperparameters not greater than 6)
- Plot, ACF and PACF of residuals
- Ljung-Box Q statistics to test if autocorrelation coefficients are zero(equivalent to F statistic for linear regression)

Simple Exponential Smoothing

- When SARIMA fails
- $s_t = \alpha x_t + (1-\alpha) s_{t-1}$ 
- HoltWinters

Further reading:

- Time series analysis and its applications, Shumway & Stoffer

# Reinforcement learning

- $S$ is a set of states s
- $A$ is a set of actions a
- $P_{sa}(s')$ is the state transition probability of s, a -> s'
- $\gamma$ is the discount factor
- $R(s)$ is the reward function
- $\pi(s)$ is a policy, associating an action a to each state s

The dynamic is:

$$s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} s_3 \enspace ...$$

The value of a policy is:

$$V^{\pi}(s) = E[\sum_{t=0}^{\infty} \gamma^t R(s_t) | s_0 = s, \pi]$$

and satisfies the bellman equation:

$$V^{\pi}(s) = R(s) + \gamma E_{Ps\pi}[V^{\pi}(s')]$$

In a finite state the bellman equation for every state can be used to solve efficiently for $V^{\pi}$.

There are 2 main algo for finding the optimal value function $\max_{\pi} V^{\pi}(s)$:

Value iteration:

1. $V(s) := 0$
1. Repeat for every state until convergence:
    - $V(s) := R(s) + \max\limits_a \gamma E_{Psa}[V]$

Policy iteration:

1. Initialize $\pi$ randomly
1. Repeat until convergence:
    1. $V := V^{\pi}$
    1. For each state, $\pi(s) = arg \max\limits_a E_{Psa}[V]$

If the model P is unknown it can be learned from experiments:

$$P_{sa}(s') = \text{frequency of s' when starting from s and taking action a}$$

If the reward $R(s)$ is unknown it can also be estimated by the average reward in state s.

Finally an algorithm for finite state MDP with unknown model is:

1. Initialize $\pi$ randomly
1. Repeat until convergence:
    1. Execute $\pi$ for some number of trials
    1. Update the estimates for $P_{sa}$
    1. Apply value iteration to get an estimate of $V$
    1. Update $\pi$ to be the greedy policy for $V$

If the state space is continuous and low dimensional, it can be discretized and we fall back to the finite space case.

If the state space is continuous and high dimensional we can approximate the value function by supervised learning. In this case a model P or simulator of P is important, as we need a lot of samples to train. The simulator can be based on the laws of physics (for a physical problem) or learned from experience by supervised learning. 

[http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes12.pdf]


# Stanford CS229

[Linear regression, logistic regression, GLMs](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes1.pdf)

[Generative models, GDA, Naive Bayes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes2.pdf)

[SVM, kernels, lagrange duality](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes3.pdf)

[Learning theory](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes4.pdf)

[Regularization and model selection](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes5.pdf)

[The perceptron and large margin classifiers](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes6.pdf)

[The k-means clustering algorithm](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes7a.pdf)

[Mixtures of Gaussians and the EM algorithm](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes7b.pdf)

[The EM algorithm](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes8.pdf)

[Factor analysis](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes9.pdf)

[Principal components analysis](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes10.pdf)

[Independent Components Analysis](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes11.pdf)

[Reinforcement Learning and Control](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes12.pdf)

[LQR, DDP and LQG](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes13.pdf)

[Bias-Variance and Error Analysis*](http://cs229.stanford.edu/notes/cs229-notes-all/error-analysis.pdf)

[Hoeffdingâ€™s inequality*](http://cs229.stanford.edu/notes/cs229-notes-all/hoeffding.pdf)

[Loss functions*](http://cs229.stanford.edu/notes/cs229-notes-all/loss-functions.pdf)

[Representer theorem*](http://cs229.stanford.edu/notes/cs229-notes-all/representer-function.pdf)


